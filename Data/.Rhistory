#save search results into data frame
for(i in 0:maxPages){
#get the search results of each page
tryCatch({
nytSearch = fromJSON(paste0(url, "&page=", i), flatten = T)
temp = data.frame(id=1:nrow(nytSearch$response$docs),
created_time = nytSearch$response$docs$pub_date,
#snippet = nytSearch$response$docs$snippet,
headline = nytSearch$response$docs$headline.main,
web_url = nytSearch$response$docs$web_url,
content = body_text(nytSearch$response$docs$web_url))
df=rbind(df,temp)
Sys.sleep(6) #sleep for 6 second
}, error = function(e) return(NA_character_))
}
#print(maxPages)
return(df)
}
cv = nytime("coronavirus")
write.csv(cv, "NYT news_coronavirus.csv")
rm(cv)
c19 = nytime("Covid-19")
View(nytime)
if (!require("jsonlite")) install.packages("jsonlite")
library(jsonlite)
#################################################################################
####            function - search news article with API                      ####
nytime = function (keyword) {
searchQ = URLencode(keyword)
url = paste('http://api.nytimes.com/svc/search/v2/articlesearch.json?q=',searchQ,
'&begin_date=20200201&end_date=20200430&sort=relevance&api-key=',api.key.nytimes,sep="")
#get the total number of search results
initialsearch = fromJSON(url,flatten = T)
maxPages = round((initialsearch$response$meta$hits / 10) - 1)
#try with the max page limit at 200
maxPages = ifelse(maxPages >= 199, 199, maxPages)
#creat a empty data frame
df = data.frame(id=as.numeric(),created_time=character(),snippet=character(),
headline=character())
body_text <- function(url_list) {
content_list = rep(NA, NROW(url_list))
for(i in 1:NROW(url_list)){
content_list[i] = tryCatch({
xml2::read_html(url_list[i]) %>% rvest::html_nodes(".StoryBodyCompanionColumn p") %>%
rvest::html_text() %>% paste(collapse = "\n")
}, error = function(e) return(NA_character_))
}
#print(maxPages)
return(content_list)
}
#save search results into data frame
for(i in 0:maxPages){
#get the search results of each page
tryCatch({
nytSearch = fromJSON(paste0(url, "&page=", i), flatten = T)
temp = data.frame(id=1:nrow(nytSearch$response$docs),
created_time = nytSearch$response$docs$pub_date,
#snippet = nytSearch$response$docs$snippet,
headline = nytSearch$response$docs$headline.main,
web_url = nytSearch$response$docs$web_url,
content = body_text(nytSearch$response$docs$web_url))
df=rbind(df,temp)
Sys.sleep(6) #sleep for 6 second
}, error = function(e) return(NA_character_))
}
#print(maxPages)
return(df)
}
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite)
library(tidyverse)
library(rstan)
source("api-keys.R")
if (!require("jsonlite")) install.packages("jsonlite")
library(jsonlite)
#################################################################################
####            function - search news article with API                      ####
nytime = function (keyword) {
searchQ = URLencode(keyword)
url = paste('http://api.nytimes.com/svc/search/v2/articlesearch.json?q=',searchQ,
'&begin_date=20200201&end_date=20200430&sort=relevance&api-key=',api.key.nytimes,sep="")
#get the total number of search results
initialsearch = fromJSON(url,flatten = T)
maxPages = round((initialsearch$response$meta$hits / 10) - 1)
#try with the max page limit at 200
maxPages = ifelse(maxPages >= 199, 199, maxPages)
#creat a empty data frame
df = data.frame(id=as.numeric(),created_time=character(),snippet=character(),
headline=character())
body_text <- function(url_list) {
content_list = rep(NA, NROW(url_list))
for(i in 1:NROW(url_list)){
content_list[i] = tryCatch({
xml2::read_html(url_list[i]) %>% rvest::html_nodes(".StoryBodyCompanionColumn p") %>%
rvest::html_text() %>% paste(collapse = "\n")
}, error = function(e) return(NA_character_))
}
#print(maxPages)
return(content_list)
}
#save search results into data frame
for(i in 0:maxPages){
#get the search results of each page
tryCatch({
nytSearch = fromJSON(paste0(url, "&page=", i), flatten = T)
temp = data.frame(id=1:nrow(nytSearch$response$docs),
created_time = nytSearch$response$docs$pub_date,
#snippet = nytSearch$response$docs$snippet,
headline = nytSearch$response$docs$headline.main,
web_url = nytSearch$response$docs$web_url,
content = body_text(nytSearch$response$docs$web_url))
df=rbind(df,temp)
Sys.sleep(6) #sleep for 6 second
}, error = function(e) return(NA_character_))
}
#print(maxPages)
return(df)
}
if (!require("jsonlite")) install.packages("jsonlite")
library(jsonlite)
#################################################################################
####            function - search news article with API                      ####
nytime = function (keyword) {
searchQ = URLencode(keyword)
url = paste('http://api.nytimes.com/svc/search/v2/articlesearch.json?q=',searchQ,
'&begin_date=20200201&end_date=20200430&sort=relevance&fq=document_type:"article"&api-key=',api.key.nytimes,sep="")
#get the total number of search results
initialsearch = fromJSON(url,flatten = T)
maxPages = round((initialsearch$response$meta$hits / 10) - 1)
#try with the max page limit at 200
maxPages = ifelse(maxPages >= 199, 199, maxPages)
#creat a empty data frame
df = data.frame(id=as.numeric(),created_time=character(),snippet=character(),
headline=character())
body_text <- function(url_list) {
content_list = rep(NA, NROW(url_list))
for(i in 1:NROW(url_list)){
content_list[i] = tryCatch({
xml2::read_html(url_list[i]) %>% rvest::html_nodes(".StoryBodyCompanionColumn p") %>%
rvest::html_text() %>% paste(collapse = "\n")
}, error = function(e) return(NA_character_))
}
#print(maxPages)
return(content_list)
}
#save search results into data frame
for(i in 0:maxPages){
#get the search results of each page
tryCatch({
nytSearch = fromJSON(paste0(url, "&page=", i), flatten = T)
temp = data.frame(id=1:nrow(nytSearch$response$docs),
created_time = nytSearch$response$docs$pub_date,
#snippet = nytSearch$response$docs$snippet,
headline = nytSearch$response$docs$headline.main,
web_url = nytSearch$response$docs$web_url,
content = body_text(nytSearch$response$docs$web_url))
df=rbind(df,temp)
Sys.sleep(6) #sleep for 6 second
}, error = function(e) return(NA_character_))
}
#print(maxPages)
return(df)
}
cv = nytime("coronavirus")
write.csv(cv, "NYT news_coronavirus.csv")
rm(cv)
c19 = nytime("Covid-19")
write.csv(c19, "NYT news_Covid19.csv")
rm(c19)
pd = nytime("pandemic")
write.csv(pd, "NYT news_pandemic.csv")
rm(pd)
ep = ntyime("epidemic")
ep = ntyime("epidemic")
if (!require("jsonlite")) install.packages("jsonlite")
library(jsonlite)
#################################################################################
####            function - search news article with API                      ####
nytime = function (keyword) {
searchQ = URLencode(keyword)
url = paste('http://api.nytimes.com/svc/search/v2/articlesearch.json?q=',searchQ,
'&begin_date=20200201&end_date=20200430&sort=relevance&fq=document_type:"article"&api-key=',api.key.nytimes,sep="")
#get the total number of search results
initialsearch = fromJSON(url,flatten = T)
maxPages = round((initialsearch$response$meta$hits / 10) - 1)
#try with the max page limit at 200
maxPages = ifelse(maxPages >= 199, 199, maxPages)
#creat a empty data frame
df = data.frame(id=as.numeric(),created_time=character(),snippet=character(),
headline=character())
body_text <- function(url_list) {
content_list = rep(NA, NROW(url_list))
for(i in 1:NROW(url_list)){
content_list[i] = tryCatch({
xml2::read_html(url_list[i]) %>% rvest::html_nodes(".StoryBodyCompanionColumn p") %>%
rvest::html_text() %>% paste(collapse = "\n")
}, error = function(e) return(NA_character_))
}
#print(maxPages)
return(content_list)
}
#save search results into data frame
for(i in 0:maxPages){
#get the search results of each page
tryCatch({
nytSearch = fromJSON(paste0(url, "&page=", i), flatten = T)
temp = data.frame(id=1:nrow(nytSearch$response$docs),
created_time = nytSearch$response$docs$pub_date,
#snippet = nytSearch$response$docs$snippet,
headline = nytSearch$response$docs$headline.main,
web_url = nytSearch$response$docs$web_url,
content = body_text(nytSearch$response$docs$web_url))
df=rbind(df,temp)
Sys.sleep(6) #sleep for 6 second
}, error = function(e) return(NA_character_))
}
#print(maxPages)
return(df)
}
ep = ntyime("epidemic")
ep = nytime("epidemic")
write.csv(ep, "NYT news_epidemic.csv")
rm(ep)
"coronavirus Trump" %>% URLdecode()
searchq = "coronavirus Trump" %>% URLdecode()
searchq
url = paste('http://api.nytimes.com/svc/search/v2/articlesearch.json?q=',searchq,
'&begin_date=20200201&end_date=20200430&sort=relevance&fq=document_type:"article"&api-key=',api.key.nytimes,sep="")
url
url
url %>% fromJSON %>% as.data.frame()
res = url %>% fromJSON %>% as.data.frame()
res$response.meta.hits
if (!require("jsonlite")) install.packages("jsonlite")
library(jsonlite)
#################################################################################
####            function - search news article with API                      ####
nytime = function (keyword) {
searchQ = URLencode(keyword)
url = paste('http://api.nytimes.com/svc/search/v2/articlesearch.json?q=',searchQ,
'&begin_date=20200201&end_date=20200430&sort=relevance&fq=document_type:"article"&api-key=',api.key.nytimes,sep="")
#get the total number of search results
initialsearch = fromJSON(url,flatten = T)
maxPages = round((initialsearch$response$meta$hits / 10) - 1)
#try with the max page limit at 200
maxPages = ifelse(maxPages >= 199, 199, maxPages)
#creat a empty data frame
df = data.frame(id=as.numeric(),created_time=character(),snippet=character(),
headline=character())
body_text <- function(url_list) {
content_list = rep(NA, NROW(url_list))
for(i in 1:NROW(url_list)){
content_list[i] = tryCatch({
xml2::read_html(url_list[i]) %>% rvest::html_nodes(".StoryBodyCompanionColumn p") %>%
rvest::html_text() %>% paste(collapse = "\n")
}, error = function(e) return(NA_character_))
}
#print(maxPages)
return(content_list)
}
#save search results into data frame
for(i in 0:maxPages){
#get the search results of each page
tryCatch({
nytSearch = fromJSON(paste0(url, "&page=", i), flatten = T)
temp = data.frame(id=1:nrow(nytSearch$response$docs),
created_time = nytSearch$response$docs$pub_date,
#snippet = nytSearch$response$docs$snippet,
headline = nytSearch$response$docs$headline.main,
web_url = nytSearch$response$docs$web_url,
content = body_text(nytSearch$response$docs$web_url))
df=rbind(df,temp)
Sys.sleep(6) #sleep for 6 second
}, error = function(e) return(NA_character_))
}
#print(maxPages)
return(df)
}
cv_Trump = nytime("coronavirus Trump")
write.csv(cv_Trump, "NYT news_coronavirusGov.csv")
rm(cv_Trump)
c19_Trump = nytime("Covid-19 Trump")
write.csv(c19_Trump, "NYT news_Covid19Gov.csv")
rm(c19_Trump)
pd_Trump = nytime("pandemic Trump")
write.csv(pd_Trump, "NYT news_pandemicGov.csv")
rm(pd_Trump)
ep_Trump = nytime("epidemic Trump")
write.csv(ep_Trump, "NYT news_epidemicGov.csv")
rm(ep_Trump)
cv_Trump = read.csv(file = "NYT news_coronavirusGov.csv")
head(cv_Trump)
cv_Trump = read.csv(file = "NYT news_coronavirusGov.csv")
c19_Trump = read.csv(file = "NYT news_Covid19Gov.csv")
pd_Trump = read.csv(file = "NYT news_pandemicGov.csv")
ep_Trump = read.csv(file = "NYT news_epidemicGov.csv")
NYT_Trump = rbind(cv_Trump, cv_Trump, pd_Trump, ep_Trump)
NYT_Trump %>% glimpse()
rm(cv_Trump, c19_Trump, pd_Trump, ep_Trump)
head(NYT_Trump)
View(NYT_Trump)
NYT_Trump$content[3]
NYT_Trump$content[2]
write.csv(NYT_Trump, "NYT_Trump.csv")
class(NYT_Trump)
drops = c("X","id")
NYT_Trump[, !(names(NYT_Trump) %in% drops)]
head(NYT_Trump)
drops = c("X","id")
NYT_Trump = NYT_Trump[, !(names(NYT_Trump) %in% drops)]
head(NYT_Trump)
duplicated(NYT_Trump)
NYT_Trump[duplicated(NYT_Trump),]
NYT_Trump = NYT_Trump[!duplicated(NYT_Trump),]
NYT_Trump %>% glimpse()
NYT_Trump[complete.cases(NYT_Trump),]
NYT_Trump[!complete.cases(NYT_Trump),]
na.omit(NYT_Trump)
NYT_Trump = NYT_Trump[!duplicated(NYT_Trump),] %>% na.omit()
NYT_Trump %>% glimpse()
NYT_Trump = NYT_Trump[!duplicated(NYT_Trump),] %>% na.omit()
NYT_Trump$created_time = as.Date(NYT_Trump$created_time, "%Y-%m-%d")
NYT_Trump %>% glimpse()
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite)
library(tidyverse)
library(rstan)
library(dplyr)
NYT_Trump %>% select(web_url, content) %>%
unnest_tokens(word, content) %>%
anti_join(get_stopwords(), by = "word") %>%
inner_join(get_sentiments("afinn"), by = "word") %>%
group_by(web_url) %>%
summarize(sentiment = sum(score)) %>%
left_join(NYT_Trump, by = "web_url") %>%
select(created_time, sentiment) %>%
group_by(date = created_time) %>%
summarize(sentiment = mean(sentiment), n = n())
install.packages("tidytext")
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite)
library(tidyverse)
library(rstan)
library(dplyr)
library(tidytext)
NYT_Trump %>% select(web_url, content) %>%
unnest_tokens(word, content) %>%
anti_join(get_stopwords(), by = "word") %>%
inner_join(get_sentiments("afinn"), by = "word") %>%
group_by(web_url) %>%
summarize(sentiment = sum(score)) %>%
left_join(NYT_Trump, by = "web_url") %>%
select(created_time, sentiment) %>%
group_by(date = created_time) %>%
summarize(sentiment = mean(sentiment), n = n())
NYT_Trump %>% select(web_url, content) %>%
unnest_tokens(word, content) %>%
anti_join(get_stopwords(), by = "word") %>%
inner_join(get_sentiments("afinn"), by = "word")# %>%
NYT_Trump %>% select(web_url, content) %>%
unnest_tokens(word, content) #%>%
NYT_Trump %>% select(web_url, content) %>%
head()
NYT_Trump %>% select(web_url, content) %>%
class()
NYT_Trump %>% as.tibble() %>%
select(web_url, content) %>%
class()
NYT_Trump %>% as_tibble() %>%
select(web_url, content) %>%
class()
NYT_Trump %>% as_tibble() %>%
select(web_url, content) %>%
unnest_tokens(word, content) %>%
head()
NYT_Trump$content = as.character(NYT_Trump$content)
NYT_Trump %>% as_tibble() %>%
select(web_url, content) %>%
unnest_tokens(word, content) %>%
head()
NYT_Trump %>% select(web_url, content) %>%
unnest_tokens(word, content) %>%
anti_join(get_stopwords(), by = "word") %>%
inner_join(get_sentiments("afinn"), by = "word") %>%
group_by(web_url) %>%
summarize(sentiment = sum(score)) %>%
left_join(NYT_Trump, by = "web_url") %>%
select(created_time, sentiment) %>%
group_by(date = created_time) %>%
summarize(sentiment = mean(sentiment), n = n()) %>%
head()
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite)
library(tidyverse)
library(rstan)
library(dplyr)
library(tidytext)
library(textdata)
install.packages("textdata")
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite)
library(tidyverse)
library(rstan)
library(dplyr)
library(tidytext)
library(textdata)
NYT_Trump %>% select(web_url, content) %>%
unnest_tokens(word, content) %>%
anti_join(get_stopwords(), by = "word") %>%
inner_join(get_sentiments("afinn"), by = "word") %>%
group_by(web_url) %>%
summarize(sentiment = sum(score)) %>%
left_join(NYT_Trump, by = "web_url") %>%
select(created_time, sentiment) %>%
group_by(date = created_time) %>%
summarize(sentiment = mean(sentiment), n = n()) %>%
head()
get_sentiments("afinn")
install.packages("textdata")
install.packages("textdata")
get_sentiments("afinn")
remotes::install_github("EmilHvitfeldt/textdata")
get_sentiments("afinn")
install.packages("remotes")
remotes::install_github("EmilHvitfeldt/textdata")
get_sentiments("afinn")
install.packages("dplyr")
install.packages("textdata")
setwd("~/Desktop/Rutgers/Courses/Spring 2020/597/Project/Code/data")
knitr::opts_chunk$set(echo = TRUE)
library('httr')
library(nytimes)
library(forecast)
install.packages("forecast")
library(forecast)
install.packages("forecast", dependencies = TRUE)
library(forecast)
library(importTS)
library(imputeTS)
#ibrary(forecast)
#library(imputeTS)
# This is the AirPassenger data with some observations removed
plot(tsAirgap, main="AirPassenger data with missing values")
library(imputeTS)
installed.packages("forecast")
remove.packages("forecast")
setwd()
library(car)
car
data(car)
data(carData)
car
library(devtools)
install_github("forecast", "robjhyndman")
remotes::install_github("robjhyndman/forecast")
library(forecast)
remotes::install_github("EmilHvitfeldt/textdata")
library(textdata)
my_lexicon = lexicon_afinn()
library(ggplot2)
library(forecast)
remove.packages(forecast)
remove.packages("forecast")
remotes::install_github("robjhyndman/forecast")
install.packages("RcppArmadillo")
library(RcppArmadillo)
install.packages("RcppArmadillo", dependencies = TRUE)
library(RcppArmadillo)
library(imputeTS)
install.packages("forecast", dependencies = TRUE)
library(forecast)
library(imputeTS)
#ibrary(forecast)
#library(imputeTS)
# This is the AirPassenger data with some observations removed
plot(tsAirgap, main="AirPassenger data with missing values")
install.packages("forecast")
install.packages("forecast")
polls_selected
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite)
library(tidyverse)
install.packages("tidyverse")
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite)
library(tidyverse)
install.packages("broom")
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite)
library(tidyverse)
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite)
library(tidyverse)
remove.packages("broom")
install.packages("tidyverse")
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite)
library(tidyverse)
install.packages("tidymodels")
install.packages("broom")
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite)
library(tidyverse)
